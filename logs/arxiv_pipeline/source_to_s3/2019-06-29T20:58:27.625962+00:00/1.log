[2019-07-01 20:58:30,869] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: arxiv_pipeline.source_to_s3 2019-06-29T20:58:27.625962+00:00 [queued]>
[2019-07-01 20:58:30,873] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: arxiv_pipeline.source_to_s3 2019-06-29T20:58:27.625962+00:00 [queued]>
[2019-07-01 20:58:30,873] {__init__.py:1353} INFO - 
--------------------------------------------------------------------------------
[2019-07-01 20:58:30,873] {__init__.py:1354} INFO - Starting attempt 1 of 2
[2019-07-01 20:58:30,873] {__init__.py:1355} INFO - 
--------------------------------------------------------------------------------
[2019-07-01 20:58:30,883] {__init__.py:1374} INFO - Executing <Task(PythonOperator): source_to_s3> on 2019-06-29T20:58:27.625962+00:00
[2019-07-01 20:58:30,883] {base_task_runner.py:119} INFO - Running: ['airflow', 'run', 'arxiv_pipeline', 'source_to_s3', '2019-06-29T20:58:27.625962+00:00', '--job_id', '2', '--raw', '-sd', 'DAGS_FOLDER/load.py', '--cfg_path', '/tmp/tmppqxf3sl4']
[2019-07-01 20:58:31,466] {base_task_runner.py:101} INFO - Job 2: Subtask source_to_s3 [2019-07-01 20:58:31,465] {__init__.py:51} INFO - Using executor SequentialExecutor
[2019-07-01 20:58:31,696] {base_task_runner.py:101} INFO - Job 2: Subtask source_to_s3 [2019-07-01 20:58:31,695] {__init__.py:305} INFO - Filling up the DagBag from /home/kent/airflow/dags/load.py
[2019-07-01 20:58:31,965] {base_task_runner.py:101} INFO - Job 2: Subtask source_to_s3 [2019-07-01 20:58:31,965] {cli.py:517} INFO - Running <TaskInstance: arxiv_pipeline.source_to_s3 2019-06-29T20:58:27.625962+00:00 [running]> on host Aspire-E5-572G
[2019-07-01 20:58:31,974] {python_operator.py:104} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_ID=arxiv_pipeline
AIRFLOW_CTX_TASK_ID=source_to_s3
AIRFLOW_CTX_EXECUTION_DATE=2019-06-29T20:58:27.625962+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2019-06-29T20:58:27.625962+00:00
[2019-07-01 20:58:36,752] {python_operator.py:113} INFO - Done. Returned value was: None
