[2019-07-01 17:13:23,218] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: arxiv_pipeline.source_to_s3 2019-07-01T17:25:12.552864+00:00 [queued]>
[2019-07-01 17:13:23,222] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: arxiv_pipeline.source_to_s3 2019-07-01T17:25:12.552864+00:00 [queued]>
[2019-07-01 17:13:23,222] {__init__.py:1353} INFO - 
--------------------------------------------------------------------------------
[2019-07-01 17:13:23,222] {__init__.py:1354} INFO - Starting attempt 1 of 2
[2019-07-01 17:13:23,222] {__init__.py:1355} INFO - 
--------------------------------------------------------------------------------
[2019-07-01 17:13:23,232] {__init__.py:1374} INFO - Executing <Task(PythonOperator): source_to_s3> on 2019-07-01T17:25:12.552864+00:00
[2019-07-01 17:13:23,232] {base_task_runner.py:119} INFO - Running: ['airflow', 'run', 'arxiv_pipeline', 'source_to_s3', '2019-07-01T17:25:12.552864+00:00', '--job_id', '28', '--raw', '-sd', 'DAGS_FOLDER/load.py', '--cfg_path', '/tmp/tmp805egta0']
[2019-07-01 17:13:23,933] {base_task_runner.py:101} INFO - Job 28: Subtask source_to_s3 [2019-07-01 17:13:23,933] {__init__.py:51} INFO - Using executor SequentialExecutor
[2019-07-01 17:13:24,200] {base_task_runner.py:101} INFO - Job 28: Subtask source_to_s3 [2019-07-01 17:13:24,200] {__init__.py:305} INFO - Filling up the DagBag from /home/kent/airflow/dags/load.py
[2019-07-01 17:13:24,491] {base_task_runner.py:101} INFO - Job 28: Subtask source_to_s3 [2019-07-01 17:13:24,491] {cli.py:517} INFO - Running <TaskInstance: arxiv_pipeline.source_to_s3 2019-07-01T17:25:12.552864+00:00 [running]> on host Aspire-E5-572G
[2019-07-01 17:13:24,502] {python_operator.py:104} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_ID=arxiv_pipeline
AIRFLOW_CTX_TASK_ID=source_to_s3
AIRFLOW_CTX_EXECUTION_DATE=2019-07-01T17:25:12.552864+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2019-07-01T17:25:12.552864+00:00
[2019-07-01 17:13:26,827] {__init__.py:1580} ERROR - arxiv-pipeline/arxiv.csv
Traceback (most recent call last):
  File "/home/kent/.local/lib/python3.6/site-packages/pandas/io/s3.py", line 30, in get_filepath_or_buffer
    filepath_or_buffer = fs.open(_strip_schema(filepath_or_buffer), mode)
  File "/home/kent/.local/lib/python3.6/site-packages/s3fs/core.py", line 352, in open
    s3_additional_kwargs=kw)
  File "/home/kent/.local/lib/python3.6/site-packages/s3fs/core.py", line 1210, in __init__
    info = self.info()
  File "/home/kent/.local/lib/python3.6/site-packages/s3fs/core.py", line 1228, in info
    refresh=refresh, **kwargs)
  File "/home/kent/.local/lib/python3.6/site-packages/s3fs/core.py", line 526, in info
    Key=key, **self.req_kw)
  File "/home/kent/.local/lib/python3.6/site-packages/s3fs/core.py", line 195, in _call_s3
    return method(**additional_kwargs)
  File "/home/kent/.local/lib/python3.6/site-packages/botocore/client.py", line 357, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/home/kent/.local/lib/python3.6/site-packages/botocore/client.py", line 648, in _make_api_call
    operation_model, request_dict, request_context)
  File "/home/kent/.local/lib/python3.6/site-packages/botocore/client.py", line 667, in _make_request
    return self._endpoint.make_request(operation_model, request_dict)
  File "/home/kent/.local/lib/python3.6/site-packages/botocore/endpoint.py", line 102, in make_request
    return self._send_request(request_dict, operation_model)
  File "/home/kent/.local/lib/python3.6/site-packages/botocore/endpoint.py", line 132, in _send_request
    request = self.create_request(request_dict, operation_model)
  File "/home/kent/.local/lib/python3.6/site-packages/botocore/endpoint.py", line 116, in create_request
    operation_name=operation_model.name)
  File "/home/kent/.local/lib/python3.6/site-packages/botocore/hooks.py", line 356, in emit
    return self._emitter.emit(aliased_event_name, **kwargs)
  File "/home/kent/.local/lib/python3.6/site-packages/botocore/hooks.py", line 228, in emit
    return self._emit(event_name, kwargs)
  File "/home/kent/.local/lib/python3.6/site-packages/botocore/hooks.py", line 211, in _emit
    response = handler(**kwargs)
  File "/home/kent/.local/lib/python3.6/site-packages/botocore/signers.py", line 90, in handler
    return self.sign(operation_name, request)
  File "/home/kent/.local/lib/python3.6/site-packages/botocore/signers.py", line 157, in sign
    auth.add_auth(request)
  File "/home/kent/.local/lib/python3.6/site-packages/botocore/auth.py", line 425, in add_auth
    super(S3SigV4Auth, self).add_auth(request)
  File "/home/kent/.local/lib/python3.6/site-packages/botocore/auth.py", line 357, in add_auth
    raise NoCredentialsError
botocore.exceptions.NoCredentialsError: Unable to locate credentials

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/kent/.local/lib/python3.6/site-packages/s3fs/core.py", line 526, in info
    Key=key, **self.req_kw)
  File "/home/kent/.local/lib/python3.6/site-packages/s3fs/core.py", line 195, in _call_s3
    return method(**additional_kwargs)
  File "/home/kent/.local/lib/python3.6/site-packages/botocore/client.py", line 357, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/home/kent/.local/lib/python3.6/site-packages/botocore/client.py", line 661, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/kent/.local/lib/python3.6/site-packages/airflow/models/__init__.py", line 1441, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/home/kent/.local/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 112, in execute
    return_value = self.execute_callable()
  File "/home/kent/.local/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 117, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/kent/airflow/dags/load.py", line 28, in source_to_s3
    old_feed = pd.read_csv(destination)
  File "/home/kent/.local/lib/python3.6/site-packages/pandas/io/parsers.py", line 702, in parser_f
    return _read(filepath_or_buffer, kwds)
  File "/home/kent/.local/lib/python3.6/site-packages/pandas/io/parsers.py", line 413, in _read
    filepath_or_buffer, encoding, compression)
  File "/home/kent/.local/lib/python3.6/site-packages/pandas/io/common.py", line 216, in get_filepath_or_buffer
    mode=mode)
  File "/home/kent/.local/lib/python3.6/site-packages/pandas/io/s3.py", line 39, in get_filepath_or_buffer
    filepath_or_buffer = fs.open(_strip_schema(filepath_or_buffer), mode)
  File "/home/kent/.local/lib/python3.6/site-packages/s3fs/core.py", line 352, in open
    s3_additional_kwargs=kw)
  File "/home/kent/.local/lib/python3.6/site-packages/s3fs/core.py", line 1210, in __init__
    info = self.info()
  File "/home/kent/.local/lib/python3.6/site-packages/s3fs/core.py", line 1228, in info
    refresh=refresh, **kwargs)
  File "/home/kent/.local/lib/python3.6/site-packages/s3fs/core.py", line 537, in info
    raise_from(FileNotFoundError(path), e)
  File "<string>", line 3, in raise_from
FileNotFoundError: arxiv-pipeline/arxiv.csv
[2019-07-01 17:13:26,833] {__init__.py:1603} INFO - Marking task as UP_FOR_RETRY
